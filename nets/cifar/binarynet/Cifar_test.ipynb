{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfbbc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 10:03:24.039247: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-25 10:03:24.164232: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-25 10:03:24.164249: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-25 10:03:24.189555: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-25 10:03:24.796601: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 10:03:24.796653: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 10:03:24.796659: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-25 10:03:27.279017: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-25 10:03:27.279043: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-25 10:03:27.279062: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Mimic): /proc/driver/nvidia/version does not exist\n",
      "2023-03-25 10:03:27.279262: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+sequential stats------------------------------------------------------------------------------+\n",
      "| Layer                  Input prec.            Outputs  # 2-bit  # 32-bit  Memory  2-bit MACs |\n",
      "|                              (bit)                         x 1       x 1    (kB)             |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "| quant_conv2d                     1   (-1, 32, 32, 64)     1728         0    0.42     1769472 |\n",
      "| batch_normalization              -   (-1, 32, 32, 64)        0       128    0.50           0 |\n",
      "| quant_conv2d_1                   1   (-1, 32, 32, 64)    36864         0    9.00    37748736 |\n",
      "| batch_normalization_1            -   (-1, 32, 32, 64)        0       128    0.50           0 |\n",
      "| max_pooling2d                    -   (-1, 16, 16, 64)        0         0       0           0 |\n",
      "| quant_conv2d_2                   1  (-1, 16, 16, 128)    73728         0   18.00    18874368 |\n",
      "| batch_normalization_2            -  (-1, 16, 16, 128)        0       256    1.00           0 |\n",
      "| quant_conv2d_3                   1  (-1, 16, 16, 128)   147456         0   36.00    37748736 |\n",
      "| batch_normalization_3            -  (-1, 16, 16, 128)        0       256    1.00           0 |\n",
      "| max_pooling2d_1                  -    (-1, 8, 8, 128)        0         0       0           0 |\n",
      "| quant_conv2d_4                   1    (-1, 8, 8, 256)   294912         0   72.00    18874368 |\n",
      "| batch_normalization_4            -    (-1, 8, 8, 256)        0       512    2.00           0 |\n",
      "| quant_conv2d_5                   1    (-1, 8, 8, 256)   589824         0  144.00    37748736 |\n",
      "| batch_normalization_5            -    (-1, 8, 8, 256)        0       512    2.00           0 |\n",
      "| max_pooling2d_2                  -    (-1, 4, 4, 256)        0         0       0           0 |\n",
      "| flatten                          -         (-1, 4096)        0         0       0           0 |\n",
      "| quant_dense                      1          (-1, 512)  2097152         0  512.00     2097152 |\n",
      "| batch_normalization_6            -          (-1, 512)        0      1024    4.00           0 |\n",
      "| dropout                          -          (-1, 512)        0         0       0           ? |\n",
      "| quant_dense_1                    1          (-1, 512)   262144         0   64.00      262144 |\n",
      "| batch_normalization_7            -          (-1, 512)        0      1024    4.00           0 |\n",
      "| dropout_1                        -          (-1, 512)        0         0       0           ? |\n",
      "| quant_dense_2                    1           (-1, 10)     5120         0    1.25        5120 |\n",
      "| batch_normalization_8            -           (-1, 10)        0        20    0.08           0 |\n",
      "| activation                       -           (-1, 10)        0         0       0           ? |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "| Total                                                  3508928      3860  871.75   155128832 |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "+sequential summary-----------------------------+\n",
      "| Total params                       3.51 M     |\n",
      "| Trainable params                   3.51 M     |\n",
      "| Non-trainable params               3.86 k     |\n",
      "| Model size                         871.75 KiB |\n",
      "| Model size (8-bit FP weights)      860.44 KiB |\n",
      "| Float-32 Equivalent                13.40 MiB  |\n",
      "| Compression Ratio of Memory        0.06       |\n",
      "| Number of MACs                     155 M      |\n",
      "| Ratio of MACs that are ternarized  1.0000     |\n",
      "+-----------------------------------------------+\n",
      "******************** BEST *******************************\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 10:03:27.844828: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open halfsame_best: FAILED_PRECONDITION: halfsame_best; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n",
      "2023-03-25 10:03:28.219376: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 614400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 117s 74ms/step - loss: 1.4492 - accuracy: 0.5730\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 1.4662 - accuracy: 0.5200\n",
      "Test\n",
      "282/282 [==============================] - 24s 86ms/step - loss: 1.6126 - accuracy: 0.5437\n",
      "Val\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 1.4764 - accuracy: 0.5790\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # BinaryNet on CIFAR10\n",
    "# \n",
    "# <a href=\"https://colab.research.google.com/github/larq/docs/blob/master/docs/larq/tutorials/binarynet_cifar10.ipynb\"><button class=\"notebook-badge\">Run on Colab</button></a> <a href=\"https://github.com/larq/docs/blob/master/docs/larq/tutorials/binarynet_cifar10.ipynb\"><button class=\"notebook-badge\">View on GitHub</button></a>\n",
    "# \n",
    "# In this example we demonstrate how to use Larq to build and train BinaryNet on the CIFAR10 dataset to achieve a validation accuracy approximately 83% on laptop hardware.\n",
    "# On a Nvidia GTX 1050 Ti Max-Q it takes approximately 200 minutes to train. For simplicity, compared to the original papers [BinaryConnect: Training Deep Neural Networks with binary weights during propagations](https://arxiv.org/abs/1511.00363), and [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/abs/1602.02830), we do not impliment learning rate scaling, or image whitening.\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import larq as lq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ## Import CIFAR10 Dataset\n",
    "# \n",
    "# We download and normalize the CIFAR10 dataset.\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_images = train_images.reshape((50000, 32, 32, 3)).astype(\"float32\")\n",
    "test_images = test_images.reshape((10000, 32, 32, 3)).astype(\"float32\")\n",
    "\n",
    "# Normalize pixel values to be between -1 and 1\n",
    "train_images, test_images = train_images / 127.5 - 1, test_images / 127.5 - 1\n",
    "\n",
    "val_images, val_labels = test_images[:1000], test_labels[:1000]\n",
    "test_images, test_labels = test_images[1000:], test_labels[1000:]\n",
    "\n",
    "# ## Build BinaryNet\n",
    "# \n",
    "# Here we build the BinaryNet model layer by layer using the [Keras Sequential API](https://www.tensorflow.org/guide/keras).\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# All quantized layers except the first will use the same options\n",
    "kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_tern\",\n",
    "              kernel_constraint=\"weight_clip\",\n",
    "              use_bias=False)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # In the first layer we only quantize the weights and not the input\n",
    "    lq.layers.QuantConv2D(128, 3, input_shape=(32, 32, 3), padding=\"same\", **kwargs),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "\n",
    "    lq.layers.QuantConv2D(128, 3, padding=\"same\", **kwargs),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "\n",
    "    lq.layers.QuantConv2D(256, 3, padding=\"same\", **kwargs),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "\n",
    "    lq.layers.QuantConv2D(256, 3, padding=\"same\", **kwargs),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "\n",
    "    lq.layers.QuantConv2D(512, 3, padding=\"same\", **kwargs),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "\n",
    "    lq.layers.QuantConv2D(512, 3, padding=\"same\", **kwargs),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    lq.layers.QuantDense(1024, **kwargs),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    lq.layers.QuantDense(1024, **kwargs),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    lq.layers.QuantDense(10, **kwargs),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.999, scale=False),\n",
    "    tf.keras.layers.Activation(\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# One can output a summary of the model:\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "lq.models.summary(model)\n",
    "\n",
    "\n",
    "# ## Model Training\n",
    "# \n",
    "# Compile the model and train the model\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "eval_only = True\n",
    "if eval_only:\n",
    "    model.load_weights('fullsame_best')\n",
    "    print(\"******************** BEST *******************************\")\n",
    "    print(\"Training\")\n",
    "    model.evaluate(train_images, train_labels)\n",
    "    model.evaluate(train_images[:100], train_labels[:100])\n",
    "    print(\"Test\")\n",
    "    model.evaluate(test_images, test_labels)\n",
    "    print(\"Val\")\n",
    "    model.evaluate(val_images, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c14dfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 10:05:52.666086: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open halfsame_best: FAILED_PRECONDITION: halfsame_best; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 91ms/step\n",
      "Layer 1 Conv 0.0\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Layer 1 Sign 0.0\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Layer 2 Conv 0.0\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "Layer 2 Sign 0.0\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f24ec0cfd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "Layer 3 Conv 0.0\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f24ec0d10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "Layer 4 Conv 0.0\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "Layer 5 Conv 0.0\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "Layer 6 Conv 0.0\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "Layer 7 Conv 0.0\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "Layer 8 Conv 0.0\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "Layer 9 Conv 0.0\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "model.load_weights('fullsame_best')\n",
    "\n",
    "layer_name = 'quant_conv2d'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "arr = np.loadtxt(\"conv1.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr- outtermediate_output)\n",
    "print(\"Layer 1 Conv\", np.sum(diff))\n",
    "\n",
    "layer_name = 'batch_normalization'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "outtermediate_output[outtermediate_output>=0] = 1\n",
    "outtermediate_output[outtermediate_output<0] = 0\n",
    "arr = np.loadtxt(\"sign1.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr- outtermediate_output)\n",
    "print(\"Layer 1 Sign\", np.sum(diff))\n",
    "\n",
    "layer_name = 'quant_conv2d_1'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "arr = np.loadtxt(\"conv2.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr- outtermediate_output)\n",
    "print(\"Layer 2 Conv\", np.sum(diff))\n",
    "\n",
    "layer_name = 'batch_normalization_1'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "outtermediate_output[outtermediate_output>=0] = 1\n",
    "outtermediate_output[outtermediate_output<0] = 0\n",
    "arr = np.loadtxt(\"sign2.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr - outtermediate_output)\n",
    "print(\"Layer 2 Sign\", np.sum(diff))\n",
    "\n",
    "layer_name = 'quant_conv2d_2'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "arr = np.loadtxt(\"conv3.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr- outtermediate_output)\n",
    "print(\"Layer 3 Conv\", np.sum(diff))\n",
    "\n",
    "layer_name = 'quant_conv2d_3'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "arr = np.loadtxt(\"conv4.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr- outtermediate_output)\n",
    "print(\"Layer 4 Conv\", np.sum(diff))\n",
    "\n",
    "layer_name = 'quant_conv2d_4'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "arr = np.loadtxt(\"conv5.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr- outtermediate_output)\n",
    "print(\"Layer 5 Conv\", np.sum(diff))\n",
    "\n",
    "layer_name = 'quant_conv2d_5'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "arr = np.loadtxt(\"conv6.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr- outtermediate_output)\n",
    "print(\"Layer 6 Conv\", np.sum(diff))\n",
    "\n",
    "layer_name = 'quant_dense'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "arr = np.loadtxt(\"conv7.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr- outtermediate_output)\n",
    "print(\"Layer 7 Conv\", np.sum(diff))\n",
    "\n",
    "layer_name = 'quant_dense_1'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "arr = np.loadtxt(\"conv8.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr- outtermediate_output)\n",
    "print(\"Layer 8 Conv\", np.sum(diff))\n",
    "\n",
    "layer_name = 'quant_dense_2'\n",
    "outtermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "outtermediate_output = outtermediate_layer_model.predict(train_images[:1])\n",
    "arr = np.loadtxt(\"conv9.txt\", delimiter=\",\", dtype=int).reshape(outtermediate_output.shape)\n",
    "diff = np.abs(arr- outtermediate_output)\n",
    "print(\"Layer 9 Conv\", np.sum(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef40ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
